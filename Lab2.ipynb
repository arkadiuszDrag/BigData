{"cells":[{"cell_type":"code","source":["%scala\n//Zadanie 1\nimport org.apache.spark.sql.types.{IntegerType, StringType, StructType, StructField}\n\nval schema = StructType(Array(\n  StructField(\"imdb_title_id\", StringType, true),\n  StructField(\"ordering\", IntegerType, true),\n  StructField(\"imdb_name_id\", StringType, true),\n  StructField(\"category\", StringType, true),\n  StructField(\"job\", StringType, true),\n  StructField(\"characters\", StringType, true)))\n\nval filePath = \"dbfs:/FileStore/tables/Files/actors.csv\"\n\nval actorsDf = spark.read.format(\"csv\")\n              .option(\"header\", \"true\")\n              .option(\"inferSchema\", \"true\")\n              .load(filePath)\n\nval actorsDfSchema = spark.read.format(\"csv\")\n            .option(\"header\", \"true\")\n            .schema(schema)\n            .load(filePath)\n\n// display(actorsDf)\n// display(actorsDfSchema)\nactorsDf.printSchema\nactorsDfSchema.printSchema"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"a0c4ad5e-3836-4834-b10e-c21ad5912a6c"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[{"name":"actorsDf","typeStr":"org.apache.spark.sql.DataFrame","schema":{"type":"struct","fields":[{"name":"imdb_title_id","type":"string","nullable":true,"metadata":{}},{"name":"ordering","type":"integer","nullable":true,"metadata":{}},{"name":"imdb_name_id","type":"string","nullable":true,"metadata":{}},{"name":"category","type":"string","nullable":true,"metadata":{}},{"name":"job","type":"string","nullable":true,"metadata":{}},{"name":"characters","type":"string","nullable":true,"metadata":{}}]},"tableIdentifier":null},{"name":"actorsDfSchema","typeStr":"org.apache.spark.sql.DataFrame","schema":{"type":"struct","fields":[{"name":"imdb_title_id","type":"string","nullable":true,"metadata":{}},{"name":"ordering","type":"integer","nullable":true,"metadata":{}},{"name":"imdb_name_id","type":"string","nullable":true,"metadata":{}},{"name":"category","type":"string","nullable":true,"metadata":{}},{"name":"job","type":"string","nullable":true,"metadata":{}},{"name":"characters","type":"string","nullable":true,"metadata":{}}]},"tableIdentifier":null}],"data":"<div class=\"ansiout\">root\n |-- imdb_title_id: string (nullable = true)\n |-- ordering: integer (nullable = true)\n |-- imdb_name_id: string (nullable = true)\n |-- category: string (nullable = true)\n |-- job: string (nullable = true)\n |-- characters: string (nullable = true)\n\nroot\n |-- imdb_title_id: string (nullable = true)\n |-- ordering: integer (nullable = true)\n |-- imdb_name_id: string (nullable = true)\n |-- category: string (nullable = true)\n |-- job: string (nullable = true)\n |-- characters: string (nullable = true)\n\nimport org.apache.spark.sql.types.{IntegerType, StringType, StructType, StructField}\nschema: org.apache.spark.sql.types.StructType = StructType(StructField(imdb_title_id,StringType,true), StructField(ordering,IntegerType,true), StructField(imdb_name_id,StringType,true), StructField(category,StringType,true), StructField(job,StringType,true), StructField(characters,StringType,true))\nfilePath: String = dbfs:/FileStore/tables/Files/actors.csv\nactorsDf: org.apache.spark.sql.DataFrame = [imdb_title_id: string, ordering: int ... 4 more fields]\nactorsDfSchema: org.apache.spark.sql.DataFrame = [imdb_title_id: string, ordering: int ... 4 more fields]\n</div>","removedWidgets":[],"addedWidgets":{},"metadata":{"isDbfsCommandResult":false},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">root\n-- imdb_title_id: string (nullable = true)\n-- ordering: integer (nullable = true)\n-- imdb_name_id: string (nullable = true)\n-- category: string (nullable = true)\n-- job: string (nullable = true)\n-- characters: string (nullable = true)\n\nroot\n-- imdb_title_id: string (nullable = true)\n-- ordering: integer (nullable = true)\n-- imdb_name_id: string (nullable = true)\n-- category: string (nullable = true)\n-- job: string (nullable = true)\n-- characters: string (nullable = true)\n\nimport org.apache.spark.sql.types.{IntegerType, StringType, StructType, StructField}\nschema: org.apache.spark.sql.types.StructType = StructType(StructField(imdb_title_id,StringType,true), StructField(ordering,IntegerType,true), StructField(imdb_name_id,StringType,true), StructField(category,StringType,true), StructField(job,StringType,true), StructField(characters,StringType,true))\nfilePath: String = dbfs:/FileStore/tables/Files/actors.csv\nactorsDf: org.apache.spark.sql.DataFrame = [imdb_title_id: string, ordering: int ... 4 more fields]\nactorsDfSchema: org.apache.spark.sql.DataFrame = [imdb_title_id: string, ordering: int ... 4 more fields]\n</div>"]}}],"execution_count":0},{"cell_type":"code","source":["%scala\n// Zadanie 2\nimport org.apache.spark.sql.types._\nimport org.apache.spark.sql.functions._\nimport org.apache.spark.sql.DataFrame\n\nval actorsCols = actorsDf.select(\"category\", \"characters\")\n\nval schema = StructType(Array(\nStructField(\"category\", StringType, true),\nStructField(\"characters\", StringType, true)))\n\nval actorsJSON = actorsCols.write.option(\"header\", \"true\").json(\"dbfs:/FileStore/tables/Files/file_2.json\")\n\nval ff = spark.read.json(\"dbfs:/FileStore/tables/Files/file_2.json\")\ndisplay(ff)\n\nval actorsRows = \"\"\"\n[{\n\"imdb_title_id\": \"tt0000009\",\n\"ordering\": 1,\n\"imdb_name_id\": \"nm0063086\",\n\"category\": \"actress\",\n\"job\": \"null\",\n\"characters\": [\n\"Miss Geraldine Holbrook (Miss Jerry)\"\n]\n},\n{\n\"imdb_title_id\": \"tt0000009\",\n\"ordering\": 2,\n\"imdb_name_id\": \"nm0183823\",\n\"category\": \"actor\",\n\"job\": \"null\",\n\"characters\": [\n\"Mr. Hamilton\"\n]\n},\n{\n\"imdb_title_id\": \"tt0002844\",\n\"ordering\": 4,\n\"imdb_name_id\": \"nm0137288\",\n\"category\": \"actress\",\n\"job\": \"null\",\n\"characters\": [\n\"Lady Beltham\",\n\"maîtresse de Fantômas\"\n]\n}]\n\"\"\"\n \nval schema = StructType(Array(\nStructField(\"imdb_title_id\", StringType, true),\nStructField(\"ordering\", IntegerType, true),\nStructField(\"imdb_name_id\", StringType, true),\nStructField(\"category\", StringType, true),\nStructField(\"job\", StringType, true),\nStructField(\"characters\", StringType, true)))\n\ndef convertJSONtoDataFrame(Rows: String, schema: StructType = null): DataFrame = {\nval reader = spark.read\nOption(schema).foreach(reader.schema) //zapytac, dlaczego Option jest bez . ?\nreader.json(sc.parallelize(Array(Rows))) //tworzy RDD - https://sparkbyexamples.com/apache-spark-rdd/how-to-create-an-rdd-using-parallelize/\n}\n\nval actorsRowsDf = convertJSONtoDataFrame(actorsRows, schema)\ndisplay(actorsRowsDf)\n\nval actorsRowsDfJSON = actorsRowsDf.write.mode(\"overwrite\").json(\"dbfs:/FileStore/tables/Files/actorsRowsDfJSON.json\")\n"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"8d26446e-9c7b-4f2f-827a-315b53053c86"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"data":"","errorSummary":"","metadata":{},"errorTraceType":null,"type":"ipynbError","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>"]}}],"execution_count":0},{"cell_type":"code","source":["%scala\n//Zadanie 4\n\nval brokendata = \"\"\"\n[{\n'imdb_title_id': 'tt0110009', \n'ordering': 2, \n'imdb_name_id': 'nm003086', \n'category': 1, 'job': 'null', \n'characters': 'Miss Geraldine Holbrook (Miss Jerry)'\n},\n{\n'imdb_title_id': 123, \n'ordering': 2, \n'imdb_name_id': 'nm003086', \n'category': 1, \n'job': 'null', \n'characters': 'Miss Geraldine Holbrook (Miss Jerry)'\n},\n{\nxxxxxxxxxxxxxxxx\n}]\n\"\"\"\n \nval brokendataDf = convertJSONtoDataFrame(brokendata)\n\nval brokendataDfJSON = brokendataDf.write.mode(\"overwrite\").json(\"dbfs:/FileStore/tables/Files/brokendataDfJSON.json\")\n\nval badRecords = spark.read.format(\"json\")\n              .option(\"inferSchema\",\"true\")\n              .option(\"badRecordsPath\", \"dbfs:/FileStore/tables/Files/badrecords\")\n.load(\"dbfs:/FileStore/tables/Files/brokendataDfJSON.json\")\n\nval permissive = spark.read.format(\"json\")\n.option(\"inferSchema\",\"true\")\n.option(\"mode\", \"PERMISSIVE\")\n.load(\"dbfs:/FileStore/tables/Files/brokendataDfJSON.json\")\n  \nval dropmalformed = spark.read.format(\"json\")\n.option(\"inferSchema\",\"true\")\n.option(\"mode\", \"DROPMALFORMED\")\n.load(\"dbfs:/FileStore/tables/Files/brokendataDfJSON.json\")\n\nval failfast = spark.read.format(\"json\")\n.option(\"inferSchema\",\"true\")\n.option(\"mode\", \"FAILFAST\")\n.load(\"dbfs:/FileStore/tables/Files/brokendataDfJSON.json\")\n"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"b4ab2a12-8b1d-4a9f-b554-81431962848c"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"data":"","errorSummary":"","metadata":{},"errorTraceType":null,"type":"ipynbError","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>"]}}],"execution_count":0},{"cell_type":"code","source":["%scala\n//zadanie 5\nval file = actorsDf.write.mode(\"overwrite\").option(\"header\", \"true\").parquet(\"dbfs:/FileStore/tables/Files/file1.parquet\")\n\nval parquet_file = spark.read.format(\"parquet\")\n.option(\"inferSchema\",\"true\")\n.load(\"dbfs:/FileStore/tables/Files/file1.parquet\")\n\nval file2 = actorsDf.write.mode(\"overwrite\").option(\"header\", \"true\").json(\"dbfs:/FileStore/tables/Files/file2.json\")\n\nval json_file = spark.read.format(\"json\")\n          .option(\"inferSchema\",\"true\")\n          .load(\"dbfs:/FileStore/tables/Files/file1_2.json\")\n\ndisplay(parquet_file)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"e2eb57cb-3215-443e-a6a5-a92fcec6df37"}},"outputs":[],"execution_count":0}],"metadata":{"application/vnd.databricks.v1+notebook":{"notebookName":"Lab2","dashboards":[],"notebookMetadata":{"pythonIndentUnit":4},"language":"python","widgets":{},"notebookOrigID":2610878281203921}},"nbformat":4,"nbformat_minor":0}
